# 2.2. 词法分析器

词法分析阶段通常是编译的第一个阶段.

为什么要进行词法分析呢? 以人类的阅读来举例子: 假设你在读一部英文小说, 组成小说的最小单位是字符, 包括英文字母, 标点符号, 空白等等. 你显然不会一个字符一个字符地读这部小说, 你会尝试以空白和标点符号为界, 将字符组成一个个单词, 然后通过理解单词的含义, 来理解句子, 段落, 章节, 乃至整部小说的含义.

编译器处理源文件时也是如此: 它会尝试先把读出的字符按照某种规则, 切成一个个便于识别和处理的 token, 然后再由剩下的部分通过匹配 token 来实现后续的处理过程. 这样做会比直接在字符上进行处理要简单, 规整得多.

将输入流切成一个个 token 的过程即为词法分析, 而负责将输入流切成 token 流的工具叫做词法分析器.

本节我们将实现 `first-step` 语言的词法分析器, 即 lexer.

## token 的类型

从上一节的 [EBNF 定义](step-by-step/beginning.md?id=ebnf-格式的语法定义)中, 我们可以发现 lexer 必须能够将输入流切成如下几类 token:

* `IDENT`: 标识符, 在 `first-step` 中, 为任意以字母或下划线开头, 后接零个或多个字母, 下划线或数字的字符串.
* `INTEGER`: 整数, 即 0, 或由非零数字开头, 后接零个或多个数字的字符串.
* 关键字: `if`, `else` 和 `return`.
* 运算符: `+`, `-`, `==` 等符号.
* 其他字符: `{`, `(` 等字符.

此外, lexer 还需要跳过以下的内容:

* 空白符: 包括空格, 缩进, 换行符等符号. lexer 应当具备跳过任意多个连续的上述符号的能力.
* 注释: 以 `#` 开头, 换行符结尾, 中间包含任意多个任意字符的字符串.

当然, 可能你在初次接触编译器的实现时, 并不能立即发现上述规则. 你可以结合 EBNF 以及上一节中 `first-step` 的例程, 略加思考. 当你身经百战, 见得多了之后就会发现, 很多语言的 lexer 其实都长得差不多, token 的分类也无非就那么几种.

## 识别 token

现在我们已经知道了 token 的分类和组成规则, 那我们要如何编写 lexer, 才能让它正确的识别 token 呢?

聪明的你肯定会想到, 老师在课上曾经教过正则式, NFA, DFA, 状态转换表云云, 我们的 lexer 中一定得包含这些东西. 其实这话说得也没错, 从正则表达式匹配字符串的角度来看, 如果你把上述 token 的规则描述为正则式, 而你又期望你的 lexer 能根据这些正则式自动对输入的字符串进行高效匹配, 那你的 lexer 中最好包含这些算法和结构.

不过, 我们目前实现的 lexer 只需要解析 `first-step` 涉及的各类 token, 这些 token 的形式都是固定的, 我们并不需要设计一个通用的程序, 去把这些规则转换为自动机和转换表. 我们完全可以直接根据 token 的描述, 把匹配的逻辑写成一系列循环和条件判断, 硬编码在程序里: 这反而是最省时省力的方案, 虽然听起来这么做程序会变得很乱, 但相信我, 最终的实现一点都不乱.

### 识别标识符

啊

### 识别数字

啊

### 识别运算符

啊

### 跳过注释

啊

### 组合

啊

## 测试 lexer

啊

## 完整代码

请参考

啊
